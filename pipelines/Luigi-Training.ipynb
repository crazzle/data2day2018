{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From interactive programming to production ready code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luigi.contrib.external_program import ExternalProgramTask\n",
    "from luigi.contrib.spark import PySparkTask\n",
    "from luigi.parameter import IntParameter, Parameter\n",
    "from luigi import LocalTarget, Task\n",
    "import luigi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task No.1: Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to download the dataset using \"curl\".\n",
    "Luigi provides a baseclass named **ExternalProgramTask** to utilize external programs. \n",
    "It simply calls the external program with the provided commandline arguments. The output target can be referenced through *self.output()*.\n",
    "\n",
    "*Input*: Nothing required <br>\n",
    "*Output*: Downloaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadDataset(ExternalProgramTask):\n",
    "\n",
    "    dataset_name = Parameter(default=\"reddit_ds_got\")\n",
    "\n",
    "    base_url = \"http://plainpixels.work/resources/datasets\"\n",
    "    file_fomat = \"zip\"\n",
    "\n",
    "    def output(self):\n",
    "        return LocalTarget(\"../dataset/%s.%s\" % (self.dataset_name, self.file_fomat))\n",
    "\n",
    "    def program_args(self):\n",
    "        url = \"%s/%s.%s\" % (self.base_url, \n",
    "                            self.dataset_name, \n",
    "                            self.file_fomat)\n",
    "        return [\"curl\", \"-L\",\n",
    "                \"-o\", self.output().path,\n",
    "                url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if DownloadDataset(dataset_name=reddit_ds_got) is complete\n",
      "INFO: Informed scheduler that task   DownloadDataset_reddit_ds_got_aed65dfc23   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=730457980, workers=1, host=Marks-MacBook-Pro-2.local, username=markkeinhorster, pid=5129) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 1 tasks of which:\n",
      "* 1 present dependencies were encountered:\n",
      "    - 1 DownloadDataset(dataset_name=reddit_ds_got)\n",
      "\n",
      "Did not run any tasks\n",
      "This progress looks :) because there were no failed tasks or missing external dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luigi.build([DownloadDataset()], local_scheduler=True, no_lock=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task No.2: Extract the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before, we use **ExternalProgramTask** to unzip the archive. The major difference is that **ExtractDataset** now implements *requires(...)* and links to **DownloadDataset** as a dependency. The required target can be referenced through *self.input()*.\n",
    "\n",
    "*Input*: DownloadDataset <br>\n",
    "*Output*: A folder containing the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractDataset(ExternalProgramTask):\n",
    "    \n",
    "    dataset_name = Parameter(default=\"reddit_ds_got\")\n",
    "    \n",
    "    def requires(self):\n",
    "        return DownloadDataset(self.dataset_name)\n",
    "\n",
    "    def output(self):\n",
    "        return LocalTarget(\"datasets/%s/raw\" % self.dataset_name)\n",
    "\n",
    "    def program_args(self):\n",
    "        self.output().makedirs()\n",
    "        return [\"unzip\", \"-u\", \"-q\",\n",
    "                \"-d\", self.output().path,\n",
    "                self.input().path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if ExtractDataset(dataset_name=reddit_ds_got) is complete\n",
      "INFO: Informed scheduler that task   ExtractDataset_reddit_ds_got_aed65dfc23   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=557753216, workers=1, host=Marks-MacBook-Pro-2.local, username=markkeinhorster, pid=5129) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 1 tasks of which:\n",
      "* 1 present dependencies were encountered:\n",
      "    - 1 ExtractDataset(dataset_name=reddit_ds_got)\n",
      "\n",
      "Did not run any tasks\n",
      "This progress looks :) because there were no failed tasks or missing external dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luigi.build([ExtractDataset()], local_scheduler=True, no_lock=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task No.3: Clean the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning task, that takes care to tokenize the posts, remove stopwords and stem.\n",
    "\n",
    "*Input*: ExtractDataset <br>\n",
    "*Output*: A cleaned version of reddit posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/markkeinhorster/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/markkeinhorster/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "class Clean(Task):\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    dataset_name = Parameter(default=\"reddit_ds_got\")\n",
    "    training_data = \"training/data.csv\"\n",
    "    \n",
    "    # Der verwendete Tokenizer\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # Die Liste von Stop-Woertern\n",
    "    # die herausgefiltert werden\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "     # Der Stemmer fuer Englische Woerter\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    \n",
    "    # Als Abhaengigkeit wird der\n",
    "    # Task *Download* zurueckgegeben\n",
    "    def requires(self):\n",
    "        return ExtractDataset(self.dataset_name)\n",
    "    \n",
    "    # Das LocalTarget fuer die sauberen Daten\n",
    "    # Die Daten werden unter\n",
    "    # \"model/<version>/cleaned.csv gespeichert\n",
    "    def output(self):\n",
    "        return LocalTarget(\"datasets/%s/cleaned/cleaned.csv\" % self.dataset_name)\n",
    "\n",
    "    # Die Rohdaten werden zerstueckelt\n",
    "    # durch die Stopwort-Liste gefiltert\n",
    "    # und auf ihre Wortstaemme zurueckgefuehrt\n",
    "    def run(self):\n",
    "        import pandas\n",
    "        data = \"%s/%s\" % (self.input().path, self.training_data)\n",
    "        dataset = pandas.read_csv(data, encoding='utf-8', sep=';').fillna('')\n",
    "        dataset[\"cleaned_words\"] = dataset.apply(self.clean_words, axis=1)\n",
    "        with self.output().open(\"w\") as out:\n",
    "            dataset[[\"cleaned_words\", \"subreddit\"]].to_csv(out,  encoding='utf-8', index=False, sep=';')\n",
    "\n",
    "    def clean_words(self, post):\n",
    "        tokenized = self.tokenizer.tokenize(post[\"title\"] + \" \" + post[\"selftext\"])\n",
    "        lowercase = [word.lower() for word in tokenized]\n",
    "        filtered = [word for word in lowercase if word not in self.stopwords]\n",
    "        stemmed = [self.stemmer.stem(word) for word in filtered]\n",
    "        return \" \".join(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if Clean(dataset_name=reddit_ds_got) is complete\n",
      "DEBUG: Checking if ExtractDataset(dataset_name=reddit_ds_got) is complete\n",
      "INFO: Informed scheduler that task   Clean_reddit_ds_got_aed65dfc23   has status   PENDING\n",
      "INFO: Informed scheduler that task   ExtractDataset_reddit_ds_got_aed65dfc23   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 1\n",
      "INFO: [pid 5129] Worker Worker(salt=352907745, workers=1, host=Marks-MacBook-Pro-2.local, username=markkeinhorster, pid=5129) running   Clean(dataset_name=reddit_ds_got)\n",
      "INFO: [pid 5129] Worker Worker(salt=352907745, workers=1, host=Marks-MacBook-Pro-2.local, username=markkeinhorster, pid=5129) done      Clean(dataset_name=reddit_ds_got)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   Clean_reddit_ds_got_aed65dfc23   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=352907745, workers=1, host=Marks-MacBook-Pro-2.local, username=markkeinhorster, pid=5129) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 2 tasks of which:\n",
      "* 1 present dependencies were encountered:\n",
      "    - 1 ExtractDataset(dataset_name=reddit_ds_got)\n",
      "* 1 ran successfully:\n",
      "    - 1 Clean(dataset_name=reddit_ds_got)\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing external dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luigi.build([Clean()], local_scheduler=True, no_lock=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task No.4: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task No.4 trains the model and persists it to the filesystem.\n",
    "\n",
    "*Input*: Clean <br>\n",
    "*Output*: A file representing the model\n",
    "\n",
    "Execute from commandline:\n",
    "```bash\n",
    "PYTHONPATH='.' luigi --module 00_training_pipeline TrainModel --version 1              --local-scheduler\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel(PySparkTask):\n",
    "    dataset_name = Parameter(default=\"reddit_ds_got\")\n",
    "    version = IntParameter(default=1)\n",
    "    \n",
    "    # PySpark Parameter\n",
    "    driver_memory = '1g'\n",
    "    executor_memory = '2g'\n",
    "    executor_cores = '2'\n",
    "    num_executors = '4'\n",
    "    master = 'local'\n",
    "    \n",
    "    # Als Abhaengigkeit wird der\n",
    "    # Task *Clean* zurueckgegeben\n",
    "    def requires(self):\n",
    "        return Clean(self.dataset_name)\n",
    "    \n",
    "    # Das LocalTarget fuer das Model\n",
    "    # Die Daten werden unter\n",
    "    # \"model/<version>/model gespeichert\n",
    "    def output(self):\n",
    "        return LocalTarget(\"model/%d\" % self.version)\n",
    "\n",
    "    def main(self, sc, *args):\n",
    "        from pyspark.sql.session import SparkSession\n",
    "        from pyspark.ml import Pipeline\n",
    "        from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "        from pyspark.ml.classification import DecisionTreeClassifier\n",
    "        \n",
    "        # Initialisiere den SQLContext\n",
    "        sql = SparkSession.builder\\\n",
    "            .enableHiveSupport() \\\n",
    "            .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "            .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "            .config(\"hive.exec.max.dynamic.partitions\", \"4096\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        # Lade die bereinigten Daten\n",
    "        df = sql.read.format(\"com.databricks.spark.csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"delimiter\", \";\") \\\n",
    "            .load(self.input().path)\n",
    "        \n",
    "        # Den Klassifikator trainieren\n",
    "        labeled = df.withColumn(\"label\", df.subreddit.like(\"datascience\").cast(\"double\"))\n",
    "        train_set, test_set = labeled.randomSplit([0.8, 0.2])\n",
    "        tokenizer = Tokenizer().setInputCol(\"cleaned_words\").setOutputCol(\"tokenized\")\n",
    "        hashing = HashingTF().setNumFeatures(1000).setInputCol(\"tokenized\").setOutputCol(\"features\")\n",
    "        decision_tree = DecisionTreeClassifier()\n",
    "        pipeline = Pipeline(stages=[tokenizer, hashing, decision_tree])\n",
    "        model = pipeline.fit(train_set)\n",
    "        model.save(self.output().path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
