{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From interactive programming to production ready code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luigi.contrib.external_program import ExternalProgramTask\n",
    "from luigi.contrib.spark import PySparkTask\n",
    "from luigi.parameter import IntParameter, Parameter, DateParameter\n",
    "from luigi import LocalTarget, Task\n",
    "import luigi\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateExists(Task):\n",
    "    path = Parameter()\n",
    "\n",
    "    def output(self):\n",
    "        return LocalTarget(self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task No.1: Clean the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning task, that takes care to tokenize the posts, remove stopwords and stem.\n",
    "\n",
    "*Input*: nothing <br>\n",
    "*Output*: A cleaned version of reddit posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/markkeinhorster/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/markkeinhorster/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "class Clean(Task):\n",
    "    from datetime import date\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    # Ein Datum wird als Parameter uebergeben\n",
    "    date = DateParameter(default=date.today())\n",
    "    basepath = \"datasets/reddit_ds_got/raw\"\n",
    "    \n",
    "    # Die Liste von Stop-Woertern\n",
    "    # die herausgefiltert werden\n",
    "    stoppwoerter = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    # Der verwendete Tokenizer\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Der Stemmer fuer Englische Woerter\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    \n",
    "    def requires():\n",
    "        prefix = self.date.strftime(\"%m-%d-%Y\")\n",
    "        return DateExists(\"%s/%s/roh.csv\" % (self.basepath, prefix))\n",
    "    \n",
    "    # Das LocalTarget fuer die sauberen Daten\n",
    "    # Die Daten werden unter\n",
    "    # \"daily/<datum>/cleaned.csv gespeichert\n",
    "    def output(self):\n",
    "        prefix = self.date.strftime(\"%m-%d-%Y\")\n",
    "        return LocalTarget(\"datasets/reddit_ds_got/daily/%s/cleaned.csv\" % prefix)\n",
    "\n",
    "    # Die Rohdaten werden zerstueckelt\n",
    "    # durch die Stopwort-Liste gefiltert\n",
    "    # und auf ihre Wortstaemme zurueckgefuehrt\n",
    "    def run(self):\n",
    "        csv = self.lade()\n",
    "        tokenized = self.tokenize(csv)\n",
    "        gefiltert = self.entferne(tokenized)\n",
    "        wortstamm = self.stemme(gefiltert)\n",
    "        csv[\"cleaned_words\"] = wortstamm\n",
    "        self.speichern(csv, self.output())\n",
    "\n",
    "    def lade(self):\n",
    "        import pandas\n",
    "        prefix = self.date.strftime(\"%m-%d-%Y\")\n",
    "        path = \"%s/%s/roh.csv\" % (self.basepath, prefix)\n",
    "        dataset = pandas.read_csv(path, encoding='utf-8', sep=';').fillna('')\n",
    "        return dataset\n",
    "\n",
    "    def tokenize(self, csv):\n",
    "        def tok(post):\n",
    "            tokenized = self.tokenizer.tokenize(post[\"title\"] + \" \" + post[\"selftext\"])\n",
    "            return tokenized\n",
    "        tokenized = csv.apply(tok, axis=1)\n",
    "        return tokenized\n",
    "\n",
    "    def entferne(self, tokenized):\n",
    "        lowercase = tokenized.apply(lambda post: [wort.lower() for wort in post])\n",
    "        filtered = lowercase.apply(lambda post: [wort for wort in post if wort not in self.stoppwoerter])\n",
    "        return filtered\n",
    "\n",
    "    def stemme(self, gefiltert):\n",
    "        wortstamm = gefiltert.apply(lambda post: [self.stemmer.stem(wort) for wort in post])\n",
    "        wortstamm = wortstamm.apply(lambda post: \" \".join(post))\n",
    "        return wortstamm\n",
    "    \n",
    "    def speichern(self, dataframe, target):\n",
    "        with target.open(\"w\") as out:\n",
    "            dataframe[[\"id\", \"cleaned_words\", \"subreddit\"]].to_csv(out, encoding='utf-8', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if Clean(date=2018-02-19) is complete\n",
      "INFO: Informed scheduler that task   Clean_2018_02_19_999079b9db   has status   PENDING\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 1\n",
      "INFO: [pid 2392] Worker Worker(salt=356792662, workers=1, host=Marks-MacBook-Pro-2.local, username=markkeinhorster, pid=2392) running   Clean(date=2018-02-19)\n",
      "INFO: [pid 2392] Worker Worker(salt=356792662, workers=1, host=Marks-MacBook-Pro-2.local, username=markkeinhorster, pid=2392) done      Clean(date=2018-02-19)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   Clean_2018_02_19_999079b9db   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=356792662, workers=1, host=Marks-MacBook-Pro-2.local, username=markkeinhorster, pid=2392) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 1 tasks of which:\n",
      "* 1 ran successfully:\n",
      "    - 1 Clean(date=2018-02-19)\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing external dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luigi.build([Clean(datetime.date(2018,2,19))], local_scheduler=True, no_lock=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task No.2: Check existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task No.2 checks if the model exists.\n",
    "\n",
    "*Input*: nothing <br>\n",
    "*Output*: A file representing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelExists(Task):\n",
    "    version = IntParameter(default=1)\n",
    "\n",
    "    def output(self):\n",
    "        return LocalTarget(\"model/%d\" % self.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task No.3: Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task No.3 classifies a daily post\n",
    "\n",
    "*Input*: ModelExists, Clean <br>\n",
    "*Output*: A file with the classification results\n",
    "\n",
    "Execute from commandline:\n",
    "```bash\n",
    "PYTHONPATH='.' luigi --module 01_classification_pipeline Classify --date=2018-02-19 --local-scheduler\n",
    "\n",
    "PYTHONPATH='.' luigi --module 01_classification_pipeline RangeDailyBase --of Classify \\\n",
    "                                                                        --start=2018-02-19 \\\n",
    "                                                                        --stop=2018-02-23 \\\n",
    "                                                                        --days-back 365 \\\n",
    "                                                                        --Classify-version 1 \\\n",
    "                                                                        --reverse \\\n",
    "                                                                        --local-scheduler\n",
    "\n",
    "PYTHONPATH='.' luigi --module 01_classification_pipeline RangeDailyBase --of Classify \\\n",
    "                                                                        --stop=$(date +\"%Y-%m-%d\") \\\n",
    "                                                                        --days-back 4 \\\n",
    "                                                                        --Classify-version 1 \\\n",
    "                                                                        --reverse \\\n",
    "                                                                        --local-scheduler\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classify(PySparkTask):\n",
    "    from datetime import date\n",
    "\n",
    "    date = DateParameter(default=date.today())\n",
    "    version = IntParameter(default=1)\n",
    "\n",
    "    # PySpark Parameter\n",
    "    driver_memory = '1g'\n",
    "    executor_memory = '2g'\n",
    "    executor_cores = '2'\n",
    "    num_executors = '4'\n",
    "    master = 'local'\n",
    "\n",
    "    # Als Abhaengigkeit werden\n",
    "    # Task *Clean* und *ModelExists*\n",
    "    # zurueckgegeben\n",
    "    def requires(self):\n",
    "        return [ModelExists(self.version), Clean(self.date)]\n",
    "\n",
    "    # Das LocalTarget fuer die Klassifikation\n",
    "    # Die Daten werden unter\n",
    "    # \"daily/<datum>/ergebnis.csv gespeichert\n",
    "    def output(self):\n",
    "        prefix = self.date.strftime(\"%m-%d-%Y\")\n",
    "        return LocalTarget(\"datasets/reddit_ds_got/daily/%s/ergebnis.csv\" % prefix)\n",
    "\n",
    "    def main(self, sc, *args):\n",
    "        from pyspark.sql.session import SparkSession\n",
    "        from pyspark.ml import PipelineModel\n",
    "        from pyspark.sql.functions import when\n",
    "\n",
    "        # Initialisiere den SQLContext\n",
    "        sql = SparkSession.builder\\\n",
    "            .enableHiveSupport() \\\n",
    "            .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "            .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "            .config(\"hive.exec.max.dynamic.partitions\", \"4096\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        # Lade die bereinigten Daten\n",
    "        df = sql.read.format(\"com.databricks.spark.csv\") \\\n",
    "                     .option(\"delimiter\", \";\") \\\n",
    "                     .option(\"header\", \"true\") \\\n",
    "                     .load(self.input()[1].path)\n",
    "\n",
    "        # Lade das Model das zuvor mit SparkML trainiert wurde\n",
    "        model = PipelineModel.load(self.input()[0].path)\n",
    "\n",
    "        # Klassifiziere die Datensaetze eines Tages mit dem Model\n",
    "        ergebnis = model.transform(df)[[\"id\",\n",
    "                                        \"subreddit\",\n",
    "                                        \"probability\",\n",
    "                                        \"prediction\"]]\n",
    "\n",
    "        # Eine kleine Aufbereitung der Daten denn\n",
    "        # die Klasse \"1\" hat den Namen \"datascience\"\n",
    "        ergebnis = ergebnis.withColumn(\"prediction_label\",\n",
    "                                        when(ergebnis.prediction==1,\n",
    "                                            \"datascience\") \\\n",
    "                                        .otherwise(\"gameofthrones\"))\n",
    "\n",
    "        # Der Einfachheit halber wird der Dataframe\n",
    "        # in einen Pandas Dataframe konvertiert.\n",
    "        # Dies sollte bei grossen Datenmengen vermieden.\n",
    "        with self.output().open(\"w\") as out:\n",
    "            ergebnis.toPandas().to_csv(out,\n",
    "                                       encoding='utf-8',\n",
    "                                       index=False,\n",
    "                                       sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
